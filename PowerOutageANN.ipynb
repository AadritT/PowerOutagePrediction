{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea107c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c32a0d-c902-4c30-945b-ad184974fe38",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ann_main(df, hl_count, hl_nodes, batch_size, epochs, season_flag, thres, noise_frac, noise_var, loc):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (season_flag):\n",
    "        encoder = ce.OneHotEncoder(cols='Season',handle_unknown='return_nan', return_df = True, use_cat_names=True)\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = df.drop(columns = ['Season'])\n",
    "    \n",
    "    #the below line creates random for testing, expect bad ROC curve.\n",
    "    #df['outagePercent'] = np.random.randint(1, 101, df.shape[0])\n",
    "    df['outageValue'] = df['outageValue'].gt(thres).astype(int)\n",
    "    #print(df.loc[df['outagePercent'] > 10])\n",
    "\n",
    "    #ax = df['outagePercent'].plot.hist()\n",
    "    \n",
    "    noise_i = df.columns.get_loc(noise_var) - 1\n",
    "    \n",
    "    if (season_flag):\n",
    "        X = df.iloc[:, 1:10].values \n",
    "        y = df.iloc[:, 18].values\n",
    "        \n",
    "    else:\n",
    "        X = df.iloc[:, 1:6].values\n",
    "        y = df.iloc[:, 14].values  \n",
    "    \n",
    "    \n",
    "    #print(X.shape) \n",
    "    \n",
    "    #y = to_categorical(y, num_classes=)\n",
    "    #print(y.shape)\n",
    "    #print(y)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    \n",
    "    #print(y_train[0:100])\n",
    "    #print(\"LINE BREAK\")\n",
    "    \n",
    "    #noise_arr = np.random.normal(0, noise_frac*np.mean(X_train[:, noise_i]), (X_train.shape[0]))\n",
    "    \n",
    "    #X_train[:, noise_i] = np.add(noise_arr, X_train[:, noise_i])\n",
    "    \n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    x_row, x_col = X.shape\n",
    "    ann_input_size = x_col\n",
    "    \n",
    "\n",
    "    ann = keras.Sequential()\n",
    "    ann.add(Dense(hl_nodes, activation = 'relu', input_dim = ann_input_size))\n",
    "    for hidden_layer in range(hl_count):\n",
    "        ann.add(Dense(hl_nodes, activation = 'relu'))\n",
    "    ann.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    ann.fit(X_train, y_train, batch_size, epochs)\n",
    "\n",
    "    tr_pred = ann.predict(X_train)\n",
    "    tr_scores = ann.evaluate(X_train, y_train, verbose = 0)\n",
    "    print(\"Acc on train data: {}% \\n error on train: {}\".format(tr_scores[1], (1-tr_scores[1])))\n",
    "\n",
    "    te_scores = ann.evaluate(X_test, y_test, verbose = 0)\n",
    "    print(\"Acc on test data: {}% \\n error on test: {}\".format(te_scores[1], 1-te_scores[1]))\n",
    "\n",
    "    te_pred = ann.predict(X_test)\n",
    "    \n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, te_pred)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "    #print(fpr_keras.shape)\n",
    "    #print(tpr_keras.shape)\n",
    "    #print(thresholds_keras.shape)\n",
    "    #print(fpr_keras)\n",
    "    #print(tpr_keras)\n",
    "    #print(thresholds_keras)\n",
    "    #print(auc_keras)\n",
    "    #print(type(te_pred))\n",
    "    \n",
    "    \n",
    "    plt.figure(0)\n",
    "    plt.plot([0, 1], [0, 1], 'y--')\n",
    "    plt.plot(fpr_keras, tpr_keras, marker= \".\")\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title(loc + ' ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"../Results/Graphs/\" + loc + \"_AUC_Curve.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Y TEST\")\n",
    "    #print(y_test)\n",
    "    #print(\"TE PRED\")\n",
    "    #print(te_pred)\n",
    "    \n",
    "    # Create a Binarizer object\n",
    "    binarizer = Binarizer(threshold=0.5)\n",
    "\n",
    "    # Convert the numpy array to binary\n",
    "    te_pred_binarized = binarizer.transform(te_pred)\n",
    "    \n",
    "    #print(\"Te pred binary\")\n",
    "    #print(te_pred_binarized)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=te_pred_binarized)\n",
    "\n",
    "    cm_plot_labels = ['No outage', 'Power Outage']\n",
    "    plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=loc+' Confusion Matrix')\n",
    "\n",
    "    f1score = f1_score(y_true=y_test, y_pred=te_pred.round(), average='weighted')\n",
    "    print(\"f1Score: {}\".format(f1score))\n",
    "    precision = precision_score(y_true=y_test, y_pred=te_pred.round(), average='weighted')\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    recall = recall_score(y_true=y_test, y_pred=te_pred.round(), average='weighted')\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    \n",
    "    #for layer in ann.layers: print(layer.get_config(), layer.get_weights()) #weights\n",
    "    \n",
    "    return (te_scores[1], tr_scores[1], f1score, precision, recall, auc_keras)\n",
    "\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08f72b-58e5-4752-b173-b8fc98885660",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ann_run(filename, th_min, th_max, th_step, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step,s_flag, noise_fr_min, noise_fr_max, noise_fr_step, noise_var, loc):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df_res = pd.DataFrame(columns=['threshold', 'hl_count', 'hl_nodes','batch_size', 'epochs', 'noise_variable', 'noise_fraction', 'f1_score', 'testing_accuracy','training_accuracy', 'AUC'])\n",
    "\n",
    "    for n_fr in range(noise_fr_min, noise_fr_max+1, noise_fr_step):\n",
    "        for threshold in range(th_min, th_max+1, th_step):\n",
    "            for hl_count in range(hl_count_min,hl_count_max+1):\n",
    "                for hl_nodes in range(hl_nodes_min, hl_nodes_max+1):\n",
    "                    for batch in range(batch_min,batch_max+1):\n",
    "                        for ep in range(ep_min,ep_max+1, ep_step):\n",
    "                            te_acc, tr_acc, f1Score, precision, recall, auc = ann_main(df = df, hl_count = hl_count, hl_nodes = hl_nodes,batch_size = batch, epochs = ep, season_flag=s_flag, thres = threshold/100.0, noise_frac = n_fr/100.0,noise_var = noise_var, loc = loc)\n",
    "                            new_row = {'threshold':threshold, 'hl_count':hl_count, 'hl_nodes':hl_nodes, 'batch_size':batch, 'epochs':ep, 'noise_variable':noise_var, 'noise_fraction':n_fr, 'f1_score':f1Score, 'testing_accuracy':te_acc, 'training_accuracy':tr_acc, 'precision':precision, 'recall':recall, 'AUC': auc}\n",
    "                            df_res = df_res.append(new_row, ignore_index=True)       \n",
    "                        \n",
    "    print(\"ANN run done\")\n",
    "    \n",
    "    return (df_res)\n",
    "    \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394b6da-4645-4fce-bc2d-bb8a67ca0098",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "th_min = (int) (0.1 * 100)\n",
    "th_max = (int) (0.1 * 100)\n",
    "th_step = 1*100\n",
    "hl_count_min = 1\n",
    "hl_count_max = 1\n",
    "hl_nodes_min = 10\n",
    "hl_nodes_max= 10\n",
    "batch_min= 10\n",
    "batch_max=10\n",
    "ep_min=10 \n",
    "ep_max=10\n",
    "ep_step = 5\n",
    "s_flag = True\n",
    "noise_var = 'temp'\n",
    "noise_frac_min = 0\n",
    "noise_frac_max = 0\n",
    "noise_frac_step = 10\n",
    "df_res = ann_run(\"../Data/GeneratedData/Fresno_Weather_Outages.csv\", th_min, th_max, th_step, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, s_flag, noise_frac_min, noise_frac_max, noise_frac_step, noise_var)\n",
    "df_res.to_csv(\"../Results/Fresno_Results_20230814.csv\",index=False)\n",
    "print(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e1f77-d8a0-486c-9a94-b7dc8583eaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locationList = ['San Francisco',\n",
    "                'Fresno',\n",
    "                'San Jose',\n",
    "                'Bellevue',\n",
    "                'Eugene',\n",
    "                'Seattle',\n",
    "                'Vancouver',\n",
    "                'Tacoma',\n",
    "                'San Diego',\n",
    "                'Los Angeles'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a8a65-f3ef-462b-8697-3da3dc572def",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "df_res_com = pd.DataFrame(columns= ['location', 'f1_score', 'testing_accuracy', 'training_accuracy', 'AUC'])\n",
    "for loc in locationList:\n",
    "    th_min = (int) (0.5 * 100)\n",
    "    th_max = (int) (0.5 * 100)\n",
    "    th_step = 1000\n",
    "    hl_count_min = 1\n",
    "    hl_count_max = 1\n",
    "    hl_nodes_min = 10\n",
    "    hl_nodes_max= 10\n",
    "    batch_min= 10\n",
    "    batch_max=10\n",
    "    ep_min=10 \n",
    "    ep_max=10\n",
    "    ep_step = 5\n",
    "    s_flag = True\n",
    "    noise_var = 'temp'\n",
    "    noise_frac_min = 0\n",
    "    noise_frac_max = 0\n",
    "    noise_frac_step = 10\n",
    "    print(\"-----------------\" + loc + \"----------------------\")\n",
    "    df_res = ann_run(\"../Data/GeneratedData/\"+loc+\"_Weather_Outages.csv\", th_min, th_max, th_step, hl_count_min, hl_count_max, hl_nodes_min, hl_nodes_max, batch_min, batch_max, ep_min, ep_max, ep_step, s_flag, noise_frac_min, noise_frac_max, noise_frac_step, noise_var, loc)\n",
    "    new_row = {'location': loc, 'f1_score': df_res.loc[0, 'f1_score'], 'testing_accuracy':df_res.loc[0, 'testing_accuracy'], 'training_accuracy':df_res.loc[0, 'training_accuracy'], 'precision':df_res.loc[0, 'precision'], 'recall':df_res.loc[0, 'recall'], 'AUC':df_res.loc[0, 'AUC']}\n",
    "    df_res_com = df_res_com.append(new_row, ignore_index=True) \n",
    "    df_res.to_csv(\"../Results/ModelResults/\"+loc+\"_Results_20230903.csv\",index=False)\n",
    "    \n",
    "df_res_com = df_res_com.round(4)\n",
    "df_res_com.to_csv(\"../Results/ModelResults/Combined_20230903.csv\", index=False)\n",
    "\n",
    "df_res_com = df_res_com[['location', 'f1_score', 'testing_accuracy', 'training_accuracy', 'precision', 'recall', 'AUC']]#'AUC'\n",
    "\n",
    "\n",
    "fig = ff.create_table(df_res_com)\n",
    "fig.update_layout(\n",
    "autosize=False,\n",
    "width=750,\n",
    "height=300,\n",
    ")\n",
    "\n",
    "fig.write_image(\"../Results/ModelResults/Combined_20230903.png\", scale=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
